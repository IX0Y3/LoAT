***** FM '24 Artefact for "Accelerated Bounded Model Checking" *****

===== Download Instructions =====

You can find our artefact here:

TODO insert download link


===== Additional Requirements =====

None -- the artefact is self-contained and it does not require specific
hardware.


===== Instructions for Light Reviewing =====

Please unzip the artefact and execute:

cd ./4683/bin

To verify that the artefact can properly run, please execute:

./test

The output should look as follows:

loat_abmc_block tested successfully
loat_abmc tested successfully
loat_bmc tested successfully
loat_adcl tested successfully
spacer tested successfully
z3_bmc tested successfully
golem_tpa tested successfully
golem_bmc tested successfully
eldarica_cegar tested successfully
eldarica_sym tested successfully

To perform a short evaluation of the artefact, please run

./run_all $solver short $timeout

where $solver is the solver that should be tested, "short" is a small benchmark
collection consisting of 50 examples, and $timeout specifies the timeout per
benchmark in seconds. The available solvers are:

* loat_abmc_block
* loat_abmc
* loat_bmc
* loat_adcl
* spacer
* z3_bmc
* golem_tpa
* golem_bmc
* eldarica_cegar
* eldarica_sym

We recommend to test at least one version of each tool, e.g.,

* loat_abmc_block,
* spacer,
* golem_tpa, and
* eldarica_cegar.

(Note that Spacer is part of Z3.)

Regarding the timeout, we recommend 2 second for all solvers but Eldarica. For
Eldarica, we recommend 6 seconds. The reason is that Eldarica is implemented in
Java, and hence it can hardly solve any benchmarks within two seconds due to the
startup time of the Java Virtual Machine.

Thus, we recommend to execute the following commands for a short evaluation:

./run_all loat_abmc_block short 2
./run_all spacer short 2
./run_all golem_tpa short 2
./run_all eldarica_cegar short 6

Below, we provide the expected results for each solver, where we used a timeout
of 6 seconds for Eldarica, and 2 second for all other solvers.

* loat_abmc_block
  - unsat: 18
  - sat: 12
  - unknown: 20

* loat_abmc
  - unsat: 18
  - sat: 0
  - unknown: 32

* loat_bmc
  - unsat: 7
  - sat: 4
  - unknown: 39

* loat_adcl
  - unsat: 16
  - sat: 0
  - unknown: 34

* spacer
  - unsat: 6
  - sat: 5
  - unknown: 39

* z3_bmc
  - unsat: 5
  - sat: 0
  - unknown: 45

* golem_tpa
  - unsat: 8
  - sat: 5
  - unknown: 37

* golem_bmc
  - unsat: 6
  - sat: 0
  - unknown: 44

* eldarica_cegar
  - unsat: ?
  - sat: ?
  - unknown: ?

* eldarica_sym
  - unsat: ?
  - sat: ?
  - unknown: ?

These results were obtained on StarExec, see 4683/machine_specs.txt for
details. Of course, you might get slightly different results, depending on the
machine that you are using. If you obtain results that are significantly worse
for some solvers, then please compare your results with the detailed expected
results:

results/solved_light.txt

Then rerun the benchmarks where the results differ via

./run_$solver $benchmark

which runs the given solver with a timeout of 5 minutes. For example, on my
laptop, eldarica_cegar only proves satisfiability of 7 instead of 8 benchmarks,
so I rerun the failing benchmark via

./run_eldarica_cegar ../examples/short/chc-LIA-Lin_006.smt2

and get the expected result "sat" after ~8s.

If you obtain the expected results, then everything is fine, your machine is
just significantly slower than StarExec in these specific cases. Please take
that into account when you judge the results of further tests.


===== Detailed Instructions =====

Please unzip the artefact and execute:

cd ./4683/bin


----- available solvers -----

The artefact allows you to run the following solvers:

* loat_abmc_block -- LoAT's     implementation of ABMC with blocking clauses
* loat_abmc       -- LoAT's     implementation of ABMC without blocking clauses
* loat_bmc        -- LoAT's     implementation of BMC
* loat_adcl       -- LoAT's     implementation of ADCL
* spacer          -- Z3's       implementation of the Spacer algorithm
* z3_bmc          -- Z3's       implementation of BMC
* golem_tpa       -- Golem's    implementation of Transition Power Abstraction
* golem_bmc       -- Golem's    implementation of BMC
* eldarica_cegar  -- Eldarica's CEGAR engine
* eldarica_sym    -- Eldarica's implementation of symbolic execution


----- available benchmarks -----

The artefact contains the examples that were used for our evaluation: the
benchmarks from the CHC Competition 2023, category LIA-Lin. They are located in
the directory 4683/examples/2023. The format of the benchmarks is described
here:

https://chc-comp.github.io/format.html


----- running a single benchmark -----

You can run a single solver on a single benchmark via

./run_$solver $benchmark $timeout.

For example,

./run_loat_abmc_block ../examples/2023/chc-LIA-Lin_???.smt2 5

runs LoAT's implementation of ABMC with blocking clauses on benchmark number ???
from the CHC competition 2023 with a timeout of 5 seconds. If the timeout is
omitted, then it defaults to 300.


----- running all benchmarks -----

You can run a single solver on one of the provided collections of benchmarks via

run_all $solver $benchmarks $timeout.

For example,

./run_all loat_abmc_block 5

runs LoAT's implementation of ABMC with blocking clauses on all benchmarks with
a timeout of 5 seconds per example. If the timeout is omitted, then it defaults
to 300.

When run_all finishes, it provides the number of benchmarks where sat or unsat
was proven. For example, the command above may finish with:

unsat: 51
sat: 70
unknown: 301

Of course, you might get slightly different results, depending on the machine
that you are using.


----- reproducing our results -----

To reproduce our results for a given $solver and given $benchmarks, please run:

./run_all $solver $benchmarks

However, note that each benchmark collection contains 400-500 examples. So with
the default timeout of 300 seconds, a full run of one solver on one benchmark
collection can easily take more than one day. Thus, we recommend testing with a
smaller timeout (e.g., 5 seconds):

./run_all $solver $benchmarks $timeout

To allow for comparing the obtained results with the results from our
evaluation, we provide two csv-files:

results/unsat.csv
results/sat.csv

For each solver, they contain the number of examples where (un)satisfiability
could be proven within $timeout seconds (where 1 <= $timeout <= 300). So after
running

./run_all $solver $benchmarks $timeout,

- open the file results/(un)sat.csv (e.g., with MS Excel or LibreOffice Calc),
- within this file, search for the column $solver,
- within this column, search for the row $timeout, and
- compare the entry with the result of your run.

Of course, you might get slightly different results, depending on the machine
that you are using. We ran our tests on StarExec, see 4683/machine_specs.txt for
details.

With LibreOffice Calc, the first cell might contain "sep=,", which is just
meta-information and can be ignored.
