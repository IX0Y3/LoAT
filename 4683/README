***** TACAS '24 Artefact for "Accelerated Bounded Model Checking" *****

===== Download Instructions =====

You can find our artefact here:

TODO insert download link


===== Additional Requirements =====

None -- the artefact is self-contained and it does not require specific
hardware.


===== Instructions for Light Reviewing =====

Please unzip the artefact and execute:

cd ./4683/bin

To verify that the artefact can properly run, please execute:

./test

The output should look as follows:

loat_abmc_block tested successfully
loat_abmc tested successfully
loat_bmc tested successfully
loat_adcl tested successfully
spacer tested successfully
z3_bmc tested successfully
golem_tpa tested successfully
golem_bmc tested successfully
eldarica tested successfully

To perform a short evaluation of the artefact, please run

./run_all $solver short $timeout

where $solver is the solver that should be tested, "short" is a small benchmark
collection consisting of 50 examples, and $timeout specifies the timeout per
benchmark in seconds. The available solvers are:

* loat_abmc_block
* loat_abmc
* loat_bmc
* loat_adcl
* spacer
* z3_bmc
* golem_tpa
* golem_bmc
* eldarica

We recommend to test at least one version of each tool, e.g.,

* loat_abmc_block,
* spacer,
* golem_tpa, and
* eldarica.

(Note that Spacer is part of Z3.)

Regarding the timeout, we recommend 2 second for all solvers but eldarica. For
eldarica, we recommend 6 seconds. The reason is that eldarica is implemented in
Java, and hence it can hardly solve any benchmarks within two seconds due to the
startup time of the Java Virtual Machine.

Thus, we recommend to execute the following commands for a short evaluation:

./run_all loat_abmc_block short 2
./run_all spacer short 2
./run_all golem_tpa short 2
./run_all eldarica short 6

Below, we provide the expected results for each solver, where we used a timeout
of 6 seconds for eldarica, and 2 second for all other solvers.

* loat_abmc_block
  - unsat: 18
  - sat: 12
  - unknown: 20

* loat_abmc
  - unsat: 18
  - sat: 0
  - unknown: 32

* loat_bmc
  - unsat: 7
  - sat: 4
  - unknown: 39

* loat_adcl
  - unsat: 16
  - sat: 0
  - unknown: 34

* spacer
  - unsat: 6
  - sat: 5
  - unknown: 39

* z3_bmc
  - unsat: 5
  - sat: 0
  - unknown: 45

* golem_tpa
  - unsat: 8
  - sat: 5
  - unknown: 37

* golem_bmc
  - unsat: 6
  - sat: 0
  - unknown: 44

* eldarica
  - unsat: 3
  - sat: 8
  - unknown: 40

These results were obtained on StarExec, see 4683/machine_specs.txt for
details. Of course, you might get slightly different results, depending on the
machine that you are using. If you obtain results that are significantly worse
for some solvers, then please compare your results with the detailed expected
results:

results/solved_light.txt

Rerun those benchmarks via

./run_$solver $benchmark

which runs the given solver with a timeout of 5 minutes. For example, on my
laptop, Eldarica only proves satisfiability of 7 instead of 8 benchmarks, so I
rerun the failing benchmark via

./run_eldarica ../examples/short/chc-LIA-Lin_006.smt2

and get the expected result "sat" after ~8s.

If you obtain the expected results, then everything is fine, your machine is
just significantly slower than StarExec in these specific cases. Please take
that into account when you judge the results of further tests.

In our tests, the runtimes of Eldarica varied considerably, depending on various
factors, including the version of the JRE and whether we tested in the TACAS-AEC
virtual machine or not.


===== Detailed Instructions =====

Please unzip the artefact and execute:

cd ./4683/bin


----- available solvers -----

The artefact allows you to run the following solvers:

* loat_abmc_block -- LoAT's  implementation of ABMC with blocking clauses
* loat_abmc       -- LoAT's  implementation of ABMC without blocking clauses
* loat_bmc        -- LoAT's  implementation of BMC
* loat_adcl       -- LoAT's  implementation of ADCL
* spacer          -- Z3's    implementation of the Spacer algorithm
* z3_bmc          -- Z3's    implementation of BMC
* golem_tpa       -- Golem's implementation of Transition Power Abstraction
* golem_bmc       -- Golem's implementation of BMC
* eldarica        -- Eldarica's default configuration


----- available benchmarks -----

The artefact provides the following collections of benchmarks:

* 2022  -- the benchmarks from the CHC Competition 2022, category LIA-Lin
* 2023  -- the benchmarks from the CHC Competition 2023, category LIA-Lin

The benchmarks are located in the directories 4683/examples/2022 and
4683/examples/2023. The format of the benchmarks is described here:

https://chc-comp.github.io/format.html


----- running a single benchmark -----

You can run a single solver on a single benchmark via

./run_$solver $benchmark $timeout.

For example,

./run_loat_abmc_block ../examples/2022/chc-LIA-Lin_052.smt2 5

runs LoAT's implementation of ABMC with blocking clauses on benchmark number 52
from the CHC competition 2022 with a timeout of 5 seconds. If the timeout is
omitted, then it defaults to 300.


----- running a benchmark collection -----

You can run a single solver on one of the provided collections of benchmarks via

run_all $solver $benchmarks $timeout.

For example,

./run_all loat_abmc_block 2023 5

runs LoAT's implementation of ABMC with blocking clauses on all examples from
the benchmark collection 2023 with a timeout of 5 seconds. If the timeout is
omitted, then it defaults to 300.

When run_all finishes, it provides the number of benchmarks where sat or unsat
was proven. For example, the command above may finish with:

unsat: 51
sat: 70
unknown: 301

Of course, you might get slightly different results, depending on the machine
that you are using.


----- reproducing our results -----

To reproduce our results for a given $solver and given $benchmarks, please run:

./run_all $solver $benchmarks

However, note that each benchmark collection contains 400-500 examples. So with
the default timeout of 300 seconds, a full run of one solver on one benchmark
collection can easily take more than one day. Thus, we recommend testing with a
smaller timeout (e.g., 5 seconds):

./run_all $solver $benchmarks $timeout

To allow for comparing the obtained results with the results from our
evaluation, we provide four csv-files:

results/unsat_2022.csv
results/sat_2022.csv
results/unsat_2023.csv
results/sat_2023.csv

For each solver, they contain the number of examples from 2022 and 2023,
respectively, where (un)satisfiability could be proven within $timeout seconds
(where 1 <= $timeout <= 300). So after running

./run_all $solver $benchmarks $timeout,

- open the file results/(un)sat_$benchmarks.csv (e.g., with MS Excel or
  LibreOffice Calc),
- within this file, search for the column $solver,
- within this column, search for the row $timeout, and
- compare the entry with the result of your run.

Of course, you might get slightly different results, depending on the machine
that you are using. We ran our tests on StarExec, see 4683/machine_specs.txt for
details.

With LibreOffice Calc, the first cell might contain "sep=,", which is just
meta-information and can be ignored.
