***** TACAS '24 Artefact for "Accelerated Bounded Model Checking" *****

===== Download Instructions =====

You can find our artefact here:

TODO insert download link


===== Additional Requirements =====

None -- the artefact is self-contained and it does not require specific
hardware.


===== Instructions for Light Reviewing =====

Please unzip the artefact and execute:

cd ./4683/bin


----- Initial Testing -----

To verify that the artefact can properly run, please execute:

./test

The output should look as follows:

loat_abmc_block tested successfully
loat_abmc tested successfully
loat_bmc tested successfully
loat_adcl tested successfully
spacer tested successfully
z3_bmc tested successfully
golem_tpa tested successfully
golem_bmc tested successfully
eldarica tested successfully


----- Calibrating the Timeout -----

Please run:

./claibrate_timeout

This scripts runs several tests, measures the required time, and compares it
with the time $starexec_runtime that was required for the same tests on StarExec
in our evaluation. Then it writes a number $factor to the file
timeout_scaling_factor in the current working director. Its meaning is that the
runtime on your machine was $factor * $starexec_runtime.

In the sequel, this factor is used to scale all timeouts automatically. In this
way, we try to take the difference between the performance of your machine and
StarExec into account.


----- Short Evaluation -----

To perform a short evaluation of the artefact, please run

./run_all $solver short $timeout

where $solver is the solver that should be tested and $timeout specifies the
timeout per benchmark in seconds. The available solvers are:

* loat_abmc_block
* loat_abmc
* loat_bmc
* loat_adcl
* spacer
* z3_bmc
* golem_tpa
* golem_bmc
* eldarica

We recommend to test at least one version of each tool, e.g.,

* loat_abmc_block,
* spacer,
* golem_tpa, and
* eldarica.

(Note that Spacer is part of Z3.)

Regarding the timeout, we recommend 2 second for all solvers but eldarica. For
eldarica, we recommend 8 seconds. The reason is that eldarica is implemented in
Java, and hence it can hardly solve any benchmarks within 2 seconds due to the
startup time of the Java Virtual Machine.

Thus, we recommend to execute the following commands for a short evaluation:

./run_all loat_abmc_block short 2
./run_all spacer short 2
./run_all golem_tpa short 2
./run_all eldarica short 8

Below, we provide the expected results for each solver, where we used a timeout
of 8 seconds for eldarica, and 2 second for all other solvers. Of course, you
might get slightly different results, depending on the machine that you are
using.

* loat_abmc_block
  - unsat: 18
  - sat: 12
  - unknown: 20

* loat_abmc
  - unsat: 18
  - sat: 0
  - unknown: 32

* loat_bmc
  - unsat: 7
  - sat: 4
  - unknown: 39

* loat_adcl
  - unsat: 16
  - sat: 0
  - unknown: 34

* spacer
  - unsat: 6
  - sat: 5
  - unknown: 39

* z3_bmc
  - unsat: 5
  - sat: 0
  - unknown: 45

* golem_tpa
  - unsat: 8
  - sat: 5
  - unknown: 37

* golem_bmc
  - unsat: 6
  - sat: 0
  - unknown: 44

* eldarica
  - unsat: 3
  - sat: 8
  - unknown: 45

These results were obtained on my personal machine, see machine_specs.txt for
details.


===== Detailed Instructions =====

Please unzip the artefact and execute:

cd ./4683/bin

Then follow the instructions from the section "Calibrating the Timeout" above,
if you have not done that yet.


----- available solvers -----

The artefact allows you to run the following solvers:

* loat_abmc_block -- LoAT's  implementation of ABMC with blocking clauses
* loat_abmc       -- LoAT's  implementation of ABMC without blocking clauses
* loat_bmc        -- LoAT's  implementation of BMC
* loat_adcl       -- LoAT's  implementation of ADCL
* spacer          -- Z3's    implementation of the Spacer algorithm
* z3_bmc          -- Z3's    implementation of BMC
* golem_tpa       -- Golem's implementation of Transition Power Abstraction
* golem_bmc       -- Golem's implementation of BMC
* eldarica        -- Eldarica's default configuration


----- available benchmarks -----

The artefact provides the following collections of benchmarks:

* 2022  -- the benchmarks from the CHC Competition 2022, category LIA-Lin
* 2023  -- the benchmarks from the CHC Competition 2023, category LIA-Lin

The benchmarks are located in the directories 4683/examples/2022 and
4683/examples/2023. The format of the benchmarks is described here:

https://chc-comp.github.io/format.html


----- running a single benchmark -----

You can run a single solver on a single benchmark via

./run_$solver $benchmark $timeout.

For example,

./run_loat_abmc_block ../examples/2022/chc-LIA-Lin_052.smt2 5

runs LoAT's implementation of ABMC with blocking clauses on benchmark number 52
from the CHC competition 2022 with a timeout of 5 second. If the timeout is
omitted, then it defaults to 300.


----- running a benchmark collection -----

You can run a single solver on one of the provided collections of benchmarks via

run_all $solver $benchmarks $timeout.

For example,

./run_all loat_abmc_block 2023 5

runs LoAT's implementation of ABMC with blocking clauses on all examples from
the benchmark collection 2023 with a timeout of 5 seconds. If the timeout is
omitted, then it defaults to 300.

When run_all finishes, it provides the number of benchmarks where sat or unsat
was proven. For example, on my personal machine (see machine_specs.txt for
details) the command above finishes with:

unsat: 54
sat: 71
unknown: 297

Of course, you might get slightly different results, depending on the machine
that you are using.


----- reproducing our results -----

To reproduce our results for a given $solver and given $benchmarks, please run:

./run_all $solver $benchmarks

However, note that each benchmark collection contains 400-500 examples. So with
the default timeout of 300 seconds, a full run of one solver on one benchmark
collection can easily take more than one day. Thus, we recommend testing with a
smaller timeout (e.g., 5 seconds):

./run_all $solver $benchmarks $timeout

To allow for comparing the obtained results with the results from our
evaluation, we provide four csv-files:

results/unsat_2022.csv
results/sat_2022.csv
results/unsat_2023.csv
results/sat_2023.csv

For each solver, they contain the number of examples from 2022 and 2023,
respectively, where (un)satisfiability could be proven within $timeout seconds,
where 0 < $timeout < 300. So after running

./run_all $solver $benchmarks $timeout,

- open the file results/(un)sat_$benchmarks.csv,
- within this file, search for the column $solver,
- within this column, search for the row $timeout, and
- compare the entry with the result of your run.

Of course, you might get slightly different results, depending on the machine
that you are using. We ran our tests on StarExec. The specifications of StarExec
can be found in machine_specs.txt.

For example, on my personal machine (whose specifications can be found in
machine_specs.txt as well),

./run_all loat_abmc_block 2023 5

yields

unsat: 54
sat: 71
unknown: 297,

but the corresponding entries in results/unsat_2023.csv and results/sat_2023.csv
are 51 and 70, respectively. So on my personal machine, this solver finds
slightly more proofs than on StarExec.

