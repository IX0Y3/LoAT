***** FM '24 Artefact for "Accelerated Bounded Model Checking" *****

===== Download Instructions =====

You can find our artefact here:

TODO insert download link


===== Additional Requirements =====

None -- the artefact is self-contained and it does not require specific
hardware.


===== Instructions for Light Reviewing =====

Please unzip the artefact and execute:

cd ./abmc/bin

To verify that the artefact can properly run, please execute:

./test

The output should look as follows:

loat_abmc_b tested successfully
loat_abmc tested successfully
loat_bmc tested successfully
loat_adcl tested successfully
spacer tested successfully
z3_bmc tested successfully
golem_tpa tested successfully
golem_bmc tested successfully
eldarica_cegar tested successfully
eldarica_sym tested successfully

To perform a short evaluation of the artefact, please run

./run_all $solver $timeout short

where $solver is the solver that should be tested, "short" is a small benchmark
collection consisting of 50 examples, and $timeout specifies the timeout per
benchmark in seconds. The available solvers are:

* loat_abmc_b
* loat_abmc
* loat_bmc
* loat_adcl
* spacer
* z3_bmc
* golem_tpa
* golem_bmc
* eldarica_cegar
* eldarica_sym

We recommend to test at least one version of each tool, e.g.,

* loat_abmc_b,
* spacer,
* golem_tpa, and
* eldarica_cegar.

(Note that Spacer is part of Z3.)

Regarding the timeout, we recommend 2 second for all solvers but Eldarica. For
Eldarica, we recommend 6 seconds. The reason is that Eldarica is implemented in
Java, and hence it can hardly solve any benchmarks within two seconds due to the
startup time of the Java Virtual Machine.

Thus, we recommend to execute the following commands for a short evaluation:

./run_all loat_abmc_b 2 short
./run_all spacer 2 short
./run_all golem_tpa 2 short
./run_all eldarica_cegar 6 short

Below, we provide the expected results for each solver, where we used a timeout
of 6 seconds for Eldarica, and 2 second for all other solvers.

* loat_abmc_b
  - unsat: 18
  - sat: 12
  - unknown: 20

* loat_abmc
  - unsat: 18
  - sat: 0
  - unknown: 32

* loat_bmc
  - unsat: 7
  - sat: 4
  - unknown: 39

* loat_adcl
  - unsat: 16
  - sat: 0
  - unknown: 34

* spacer
  - unsat: 5
  - sat: 3
  - unknown: 39

* z3_bmc
  - unsat: 5
  - sat: 0
  - unknown: 45

* golem_tpa
  - unsat: 8
  - sat: 4
  - unknown: 38

* golem_bmc
  - unsat: 6
  - sat: 0
  - unknown: 44

* eldarica_cegar
  - unsat: 12
  - sat: 11
  - unknown: 27

* eldarica_sym
  - unsat: 7
  - sat: 5
  - unknown: 38

These results were obtained on StarExec, see abmc/machine_specs.txt for
details. Of course, you might get slightly different results, depending on the
machine that you are using. If you obtain results that are significantly worse
for some solvers, then please compare your results with the detailed expected
results:

results/solved_light.txt

Then rerun the benchmarks where the results differ via

./run_$solver $benchmark

which runs the given solver with a timeout of 5 minutes. For example, on my
laptop, eldarica_cegar only proves satisfiability of 7 instead of 8 benchmarks,
so I rerun the failing benchmark via

./run_eldarica_cegar ../examples/short/chc-LIA-Lin_006.smt2

and get the expected result "sat" after ~8s.

TODO check that

If you obtain the expected results, then everything is fine, your machine is
just significantly slower than StarExec in these specific cases. Please take
that into account when you judge the results of further tests.


===== Detailed Instructions =====

Please unzip the artefact and execute:

cd ./abmc/bin


----- available solvers -----

The artefact allows you to run the following solvers:

* loat_abmc_b    -- LoAT's     implementation of ABMC with blocking clauses
* loat_abmc      -- LoAT's     implementation of ABMC without blocking clauses
* loat_bmc       -- LoAT's     implementation of BMC
* loat_adcl      -- LoAT's     implementation of ADCL
* spacer         -- Z3's       implementation of the Spacer algorithm
* z3_bmc         -- Z3's       implementation of BMC
* golem_tpa      -- Golem's    implementation of Transition Power Abstraction
* golem_bmc      -- Golem's    implementation of BMC
* eldarica_cegar -- Eldarica's CEGAR engine
* eldarica_sym   -- Eldarica's implementation of symbolic execution


----- available benchmarks -----

The artefact contains the examples that were used for our evaluation: the
benchmarks from the CHC Competition 2023, category LIA-Lin. They are located in
the directory abmc/examples/2023. The format of the benchmarks is described
here:

https://chc-comp.github.io/format.html


----- running a single benchmark -----

You can run a single solver on a single benchmark via

./run_$solver $benchmark $timeout.

For example,

./run_loat_abmc_b ../examples/2023/chc-LIA-Lin_046.smt2 5

runs LoAT's implementation of ABMC with blocking clauses on benchmark number 46
from the CHC competition 2023 with a timeout of 5 seconds. If the timeout is
omitted, then it defaults to 300.


----- running all benchmarks -----

You can run a single solver on the provided collections of benchmarks via

run_all $solver $timeout.

For example,

./run_all loat_abmc_b 5

runs LoAT's implementation of ABMC with blocking clauses on all benchmarks with
a timeout of 5 seconds per example.

When run_all finishes, it provides the number of benchmarks where sat or unsat
was proven. For example, the command above may finish with:

unsat: 51
sat: 70
unknown: 301

Of course, you might get slightly different results, depending on the machine
that you are using.


----- reproducing our results -----

To reproduce our results for a given $solver, please run:

./run_all $solver 300

However, note that the provided benchmark collection contains 422 examples. So
with the timeout of 300 seconds that we used for our evaluation, a full run of
one solver can easily take more than one day. Thus, we recommend testing with a
smaller timeout, e.g., 5 seconds:

./run_all $solver 5

To allow for comparing the obtained results with the results from our
evaluation, we provide two csv-files:

results/unsat.csv
results/sat.csv

For each solver, they contain the number of examples where (un)satisfiability
could be proven within $timeout seconds (where 1 <= $timeout <= 300). So after
running

./run_all $solver $timeout,

- open the file results/(un)sat.csv (e.g., with MS Excel or LibreOffice Calc),
- within this file, search for the column $solver,
- within this column, search for the row $timeout, and
- compare the entry with the result of your run.

Of course, you might get slightly different results, depending on the machine
that you are using. We ran our tests on StarExec, see abmc/machine_specs.txt for
details.

