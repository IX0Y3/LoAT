***** FM '24 Artifact for "Integrating Loop Acceleration into Bounded Model Checking" *****

===== Getting Started =====

You can find our artifact here:

TODO insert download link

It contains a file fm24.tar, which is an archived docker image. You can load it
via:

docker image load < fm24.tar

Afterwards, please run

docker run -ti loat/fm24 /bin/bash

to start a terminal that runs in the docker container.


===== Instructions for Light Reviewing =====

To make sure that the artifact is functional, please execute:

cd /artifact/bin

To verify that the solvers can properly run, please execute:

./test

The output should look as follows:

loat_abmc_b tested successfully
loat_abmc tested successfully
loat_bmc tested successfully
loat_adcl tested successfully
spacer tested successfully
z3_bmc tested successfully
golem_tpa tested successfully
golem_bmc tested successfully
eldarica_cegar tested successfully
eldarica_sym tested successfully

To perform a short evaluation of the artifact, please run

./run_all $solver $timeout short

where $solver is the solver that should be tested, "short" is a small benchmark
collection consisting of 50 examples, and $timeout specifies the timeout per
benchmark in seconds. The available solvers are:

* loat_abmc_b
* loat_abmc
* loat_bmc
* loat_adcl
* spacer
* z3_bmc
* golem_tpa
* golem_bmc
* eldarica_cegar
* eldarica_sym

We recommend to test at least one version of each tool, e.g.,

* loat_abmc_b,
* spacer,
* golem_tpa, and
* eldarica_cegar.

(Note that Spacer is part of Z3.)

Regarding the timeout, we recommend 2 second for all solvers but Eldarica. For
Eldarica, we recommend 6 seconds. The reason is that Eldarica is implemented in
Java, and hence it can hardly solve any benchmarks within two seconds due to the
startup time of the Java Virtual Machine.

Thus, we recommend to execute the following commands for a short evaluation:

./run_all loat_abmc_b 2 short
./run_all spacer 2 short
./run_all golem_tpa 2 short
./run_all eldarica_cegar 6 short

Below, we provide the expected results for each solver, where we used a timeout
of 6 seconds for Eldarica, and 2 second for all other solvers.

* loat_abmc_b
  - unsat: 18
  - sat: 12
  - unknown: 20

* loat_abmc
  - unsat: 18
  - sat: 0
  - unknown: 32

* loat_bmc
  - unsat: 7
  - sat: 4
  - unknown: 39

* loat_adcl
  - unsat: 16
  - sat: 0
  - unknown: 34

* spacer
  - unsat: 5
  - sat: 3
  - unknown: 42

* z3_bmc
  - unsat: 5
  - sat: 0
  - unknown: 45

* golem_tpa
  - unsat: 8
  - sat: 4
  - unknown: 38

* golem_bmc
  - unsat: 6
  - sat: 0
  - unknown: 44

* eldarica_cegar
  - unsat: 12
  - sat: 11
  - unknown: 27

* eldarica_sym
  - unsat: 7
  - sat: 5
  - unknown: 38

These results were obtained on StarExec, see /artifact/machine_specs.txt for
details. Of course, you might get slightly different results, depending on the
machine that you are using. For example, golem_tpa can prove satisfiability of
5 instead of 4 instances on my machine.


===== Detailed Instructions =====

Please execute:

cd /artifact/bin


----- available solvers -----

The artifact allows you to run the following solvers:

* loat_abmc_b    -- LoAT's     implementation of ABMC with blocking clauses
* loat_abmc      -- LoAT's     implementation of ABMC without blocking clauses
* loat_bmc       -- LoAT's     implementation of BMC
* loat_adcl      -- LoAT's     implementation of ADCL
* spacer         -- Z3's       implementation of the Spacer algorithm
* z3_bmc         -- Z3's       implementation of BMC
* golem_tpa      -- Golem's    implementation of Transition Power Abstraction
* golem_bmc      -- Golem's    implementation of BMC
* eldarica_cegar -- Eldarica's CEGAR engine
* eldarica_sym   -- Eldarica's implementation of symbolic execution


----- available benchmarks -----

The artifact contains the examples that were used for our evaluation: the
benchmarks from the CHC Competition 2023, category LIA-Lin. They are located in
the directory /artifact/examples/2023. The format of the benchmarks is described
here:

https://chc-comp.github.io/format.html


----- running a single benchmark -----

You can run a single solver on a single benchmark via

./run_$solver $benchmark $timeout.

For example,

./run_loat_abmc_b ../examples/2023/chc-LIA-Lin_046.smt2 5

runs LoAT's implementation of ABMC with blocking clauses on benchmark number 46
from the CHC competition 2023 with a timeout of 5 seconds. If the timeout is
omitted, then it defaults to 300.


----- running all benchmarks -----

You can run a single solver on the provided collection of benchmarks via

./run_all $solver $timeout.

For example,

./run_all loat_abmc_b 5

runs LoAT's implementation of ABMC with blocking clauses on all benchmarks with
a timeout of 5 seconds per example.

When run_all finishes, it provides the number of benchmarks where sat or unsat
was proven. For example, the command above may finish with:

unsat: 51
sat: 70
unknown: 301

Of course, you might get slightly different results, depending on the machine
that you are using.


----- reproducing our results -----

To reproduce our results for a given $solver, please run:

./run_all $solver 300

However, note that the provided benchmark collection contains 422 examples. So
with the timeout of 300 seconds that we used for our evaluation, a full run of
one solver can easily take more than one day. Thus, we recommend testing with a
smaller timeout, e.g., 5 seconds:

./run_all $solver 5

To allow for comparing the obtained results with the results from our
evaluation, we provide two csv-files:

/artifact/results/unsat.csv
/artifact/results/sat.csv

For each solver, they contain the number of examples where (un)satisfiability
could be proven within $timeout seconds (where 1 <= $timeout <= 300). So after
running

./run_all $solver $timeout,

- open the file results/(un)sat.csv (e.g., with MS Excel or LibreOffice Calc),
- within this file, search for the column $solver,
- within this column, search for the row $timeout, and
- compare the entry with the result of your run.

Of course, you might get slightly different results, depending on the machine
that you are using. We ran our tests on StarExec, see /artifact/machine_specs.txt for
details.


----- Building LoAT -----

The sources of LoAT are available in the directory /artifact/LoAT. To build LoAT
from source, please run

cd /artifact/LoAT/build
make

resulting in the new binary:

/artifact/LoAT/build/loat-static

To run loat-static on the CHC examples in SMTLIB format, it has to be called
with the flag --format horn. Moreover, the flag --engine abmc is required to
instruct LoAT to use our implementation of ABMC. So for example, you can call:

/artifact/LoAT/build/loat-static --format horn --engine abmc /artifact/examples/2023/chc-LIA-Lin_046.smt2

See

/artifact/LoAT/build/loat-static --help

for more information about LoAT's command-line flags. The entrypoint of the ABMC
implementation is:

/artifact/LoAT/src/abmc/abmc.cpp
